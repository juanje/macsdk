# DevOps Assistant Configuration
# Copy this file to config.yml and customize as needed.
#
# This file is loaded automatically if present.
# API keys should be in .env, not here.

# =============================================================================
# LLM Configuration
# =============================================================================
# llm_model: gemini-3-flash-preview      # Model for responses
# llm_temperature: 0.3             # Creativity (0.0 - 1.0)
# llm_reasoning_effort: medium     # low, medium, high

# =============================================================================
# Middleware Configuration
# =============================================================================
# include_datetime: true           # Inject current datetime into prompts
# Note: Task planning (TODO middleware) is always enabled for all agents

# Context summarization (for long conversations)
# summarization_enabled: false
# summarization_trigger_tokens: 100000
# summarization_keep_messages: 6

# =============================================================================
# Agent Execution Configuration
# =============================================================================
# recursion_limit: 50             # Max iterations for agent tool calls
# Use higher values (100+) for complex workflows with many agents/steps

# Timeout configuration (seconds) - prevents hanging on long-running operations
# supervisor_timeout: 120.0      # Supervisor execution (includes specialist calls)
# specialist_timeout: 90.0       # Specialist agent execution (includes LLM + tools)
# formatter_timeout: 30.0        # Response formatting
# llm_request_timeout: 60.0      # Individual LLM HTTP requests

# =============================================================================
# Web Server Configuration (for macsdk web interface)
# =============================================================================
# server_host: 0.0.0.0
# server_port: 8000
# message_max_length: 5000
# warmup_timeout: 15.0

# =============================================================================
# RAG (Retrieval-Augmented Generation) Configuration
# =============================================================================
# The RAG agent indexes documentation and answers questions from it.

rag:
  enabled: true

  # Documentation sources to index
  sources:
    # MACSDK Documentation (local markdown files)
    - name: "macsdk_docs"
      url: "../../../docs/"
      type: "markdown"
      tags: ["macsdk", "sdk", "documentation"]

  # Indexing settings
  # chunk_size: 1000          # Size of text chunks
  # chunk_overlap: 200        # Overlap between chunks
  # max_depth: 3              # Crawl depth for URLs
  # embedding_model: "models/embedding-001"

  # Retrieval settings
  # retriever_k: 6            # Number of documents to retrieve
  # max_rewrites: 2           # Max query rewrites before fallback
  # model_name: "gemini-3-flash-preview"
  # temperature: 0.3

  # Caching
  # enable_llm_cache: true    # Cache LLM responses (faster, cheaper)

  # Domain-specific glossary
  glossary:
    MACSDK: "Multi-Agent Chatbot SDK - A framework for building multi-agent chatbots"
    LLM: "Large Language Model"
    RAG: "Retrieval-Augmented Generation - A technique for answering questions using indexed documents"
    LangChain: "A framework for building applications with LLMs"
    LangGraph: "A framework for building graph-based agent workflows"

  # Storage paths
  # chroma_db_dir: "./chroma_db"

# =============================================================================
# Custom Settings
# =============================================================================
# Add your chatbot-specific settings below:
# my_custom_setting: value

